<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//RU">
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=windows-1251">
<META HTTP-EQUIV="Content-Language" CONTENT="ru">
<link href="../../css/syle.css" rel="stylesheet" type="text/css"> 
<title>Лекция №1</title>
<base target="_top">
</HEAD><BODY>
<div align="center">
  <p><strong>Тема № 1 &quot;Введение. Общие сведения об информации&quot; </strong></p>
</div>

<strong> <p>Вопросы:</p> </strong>
<p>
-Общие сведения об информации. Преобразование информации. <br>
-Формы представления информации. <br>
-Меры информации.<br>
-Объем информации.
</p>
<strong> <p>Общие сведения об информации. Преобразование информации.</p> </strong>



<p>
   Любая деятельность человека представляет собой процесс сбора и переработки информации, принятия на ее основе 
решений и их выполнения. С появлением современных средств вычислительной техники информация стала выступать в качестве 
одного из важнейших ресурсов научно-технического прогресса. <br>
   По современным представлениям информация является одной из исходных категорий мироздания наряду с материей и энергией. 
Эти категории тесно взаимосвязаны между собой. Эти связи можно усмотреть и в природе и процессах, порожденных человеком.  
Пример связи этих категорий в природных явлениях:   переход жидкости их твердого состояния в жидкое – здесь есть материальные 
преобразования, энергетические затраты, а также потеря информации относительно расположения атомов. Пример связи  
материя-энергия-информация в обществе: 1. образовательный процесс – это сам по себе процесс информационный и, конечно, он 
требует материального, энергетического обеспечения; 2. любое управление, например автомобилем. Однако есть существенное отличие 
информации от материи и энергии – она может возникать и исчезать.<br>
   Невозможно ответь на вопрос – какая категория (материя, энергия или информация) важнее для человека. Вместе с тем прогресс 
человечества неизбежно влечет увеличение общего объема информации, которым оно располагает, прчем объем этот растет гораздо 
быстрее чем население земного шара и его материальные потребности. Таким образом, можно утверждать что значимость информации 
по отношению к двум другим рассмотренным категориям постепенно возрастает.<br>
   Информация содержится в человеческой речи, текстах книг, журналов, газет, сообщениях радио и телевидения, показаниях приборов 
и т.д. Человек воспринимает информацию с помощью органов чувств. Хранит и перерабатывает ее с помощью мозга и центральной нервной 
системы. Передаваемая информация обычно касается каких-то предметов или нас самих и связана с событиями, происходящими в окружающем 
нас мире. <br>
   Понятие "информация" - есть первичное и неопределяемое понятие (как, например «точка» в геометрии, «множество» в математике).<br> 
   Сам термин "информация" происходит от латинского слова informatio - разъяснение, пояснение.<br> 
   Первоначально смысл слова «информация» трактовался как нечто присущее только человеческому сознанию и общению – знания, 
сведения, известия. Затем смысл этого слова начал расширяться и обобщаться. Так, с позиций материалистической теории познания 
одним из всеобщих свойств материи (наряду с движением, развитием, пространством, временем и др.) было признано отражение, 
заключающееся в способности адекватно отображать одним реальным объектом другие реальные объекты, а сам факт отражения одного 
объекта в другом и означает присутствие в нем информации об отражаемом объекте. Таким образом, как только состояния одного объекта 
находятся в соответствии с состояниями другого объекта (например, соответствие между положением стрелки вольтметра и напряжением 
на его клеммах или соответствие между нашим ощущением и реальностью), это значит, что один объект отражает другой, т. е. содержит 
информацию о другом.<br>
   Высшей, специфической формой отражения является сознание человека.<br>
   Особенностью понятия «информация» является его универсальность - оно используется во всех без исключения сферах человеческой 
деятельности: в философии, естественных и гуманитарных науках, в биологии, медицине, в психологии человека и животных, в социологии, 
искусстве, в технике и экономике и, конечно, в повседневной жизни.<br>  
   Рассмотрим ряд других понятий, связанных с информацией.<br>
   Информация – категория нематериальная, следовательно, она должна быть связана с какой материальной основой, без этого она просто 
не сможет существовать (вспомним про изобретение письменности).<br>
   Определение. Материальный объект или среду, которые служат для представления или передачи информации, будем называть ее 
материальным носителем (бумага, диск, воздух и т.п.).<br>
   При этом хранение информации связана с фиксацией состояния носителя (например уже напечатанный текст на бумаге), а распространение 
информации – с процессом, который протекает в носителе. Но только с нестационарным  процессом, то есть характеристики которого 
меняются. (стационарный информацию не переносит – лампа просто горит и все, а мигает – уже азбука Морзе). И при этом информация 
связывается не с существованием процесса (просто горит лампа), а именно с изменением какой-либо его характеристики.<br>
   Определение. Изменение характеристики носителя, которое используется ля представления информации, называется сигналом, а значение 
этой характеристики, отнесенное к некоторой шкале измерений, называется параметром сигнала.<br></p>
<img src="../../images/lk1/1.jpg">
<p>Одиночный сигнал не может содержать много информации. Нужно их много.<br>
Определение. Последовательность сигналов называется сообщением.<br>
Сообщение, таким образом, служит переносчиком информации, а информация является содержанием сообщения.<br>
Выше уже говорилось о том, что в отличие от материи и энергии информация может создаваться и исчезать.<br>
Опр.: изменение с течением времени содержания информации или пред-ставляющего его сообщения  – это информационный процесс.<br>
   Основные виды информационных процессов:<br>
1.     создание новой информации<br>
2.     преобразование информации<br>
3.     уничтожение информации<br>
4.     передача информации<br>
Понятие «информация» обычно предполагает наличие двух объектов – источника и приемника информации. Информация передается от 
источника к приемнику в материально-энергетической форме в форме сигналов, распространяющихся в определенной среде. 
Определение. Источник информации – это субъект или объект, порождающий информацию и представляющий ее в виде сообщения. 
Определение. Получатель информации - это субъект или объект, принимающий сообщение и способный правильно его интерпретировать. 
Получатель информации не равен получателю сообщения (слышу речь на керунди – я получатель сообщения, но не информации). 
Итак, информация передается в форме сообщений от некоторого источника информации к получателю посредством системы связи между ними. 
Совокупность технических средств, используемых для передачи сообщений от источника к получателю, называется системой связи. 
Канал связи – совокупность технических устройств, обеспечивающих передачу сигнала от передатчика к приемнику.  
Кодирующее устройство предназначено для кодирования информации (преобразования исходного сообщения от источника к виду, удобному 
для передачи информации).<br>
Декодирующее устройство предназначено для преобразования полученного сообщения в исходное.<br>
Таким образом, суммируя все вышесказанное, можно сказать, что информацию нельзя считать лишь техническим термином, это фундаментальная 
философская категория, которой присущи такие свойства как запоминаемость, передаваемость, преобразуемость, воспроизводимость, 
стираемость. Можно дать следующее определение:<br>
<strong>Информация</strong> – специфический атрибут реального мира, представляющий собой его объективное отражение в виде совокупности 
сигналов и проявляющийся при взаимодействии с «приемником» информации, позволяющим выделять, регистрировать эти сигналы из окружающего 
мира и по тому или иному критерию их идентифицировать. Таким образом:<br>
1.     информация объективна, так как это свойства материи – отражение<br>
2.     информация проявляется в виде сигналов и лишь при взаимодействии объектов.<br>
3.     одна и та же информация различными получателями может быть интерпретирована по-разному.<br>
Информация имеет определенные функции и этапы обращения в обществе. Основными из них являются:<br>
1.<strong> Познавательная</strong>, цель которой — получение новой информации. Функция реализуется в основном через такие этапы 
обращения информации, как:<br>
— ее синтез (производство)<br>
— представление<br>
— хранение (передача во времени)<br>
— восприятие (потребление)<br>
2.<strong> Коммуникативная</strong> — функция общения людей, реализуемая через такие этапы обращения информации, как:<br>
— передача (в пространстве)<br>
—  распределение<br>
3.<strong> Управленческая</strong>, цель которой — формирование целесообразного поведения управляемой системы, получающей информацию.<br>
Эта функция информации неразрывно связана с познавательной и коммуникативной и реализуется через все основные этапы обращения, включая 
обработку.<br>
Без информации не может существовать жизнь в любой форме и не могут функционировать созданные человеком любые информационные системы.<br>
Без нее биологические и технические системы представляют груду химических элементов. Общение, коммуникации, обмен информацией присущи 
всем живым существам, но в особой степени — человеку. Будучи аккумулированной и обработанной с определенных позиций, информация дает 
новые сведения, приводит к новому знанию. Получение информации из окружающего мира, ее анализ и генерирование составляют одну из
основных функций человека, отличающую его от остального живого мира.<br>
<strong>Количество информации.</strong><br>
Часто приходится иметь дело с явлениями, исход которых неоднозначен и зависит от факторов, которые мы не знаем или не можем учесть.
Например – определение пола будущего ребенка, результат бросания игральной кости и пр.<br> 
События, о которых нельзя сказать произойдут они или нет, пока не будет осуществлен эксперимент, называются <strong>случайными</strong>.
Раздел математики, в котором строится понятийный и математический аппарат для описания случайных событий, называется <strong>теорией
вероятности</strong>.<br>
Осуществление некоторого комплекса условий называется опытом, а интересующий нас исход этого опыта – благоприятным событием. Тогда,
если N – общее число опытов, а N<sub>A</sub>-количество благоприятных исходов случайного события А, то отношение N/N<sub>A</sub>,
называется относительной частотой появления события А. Однако, очевидно, в разных сериях, значение частоты может оказаться различным.
Действительно, например, в серии из трех опытов по бросанию монеты может 2 раза выпасть орел и 1 раз решетка. Если благоприятным 
событием считать выпадение орла, то частота получается равно 2/3. Очевидно, что в другой серии она может быть равно 0 или 1 или 1/3.
Однако, оказывается, что при увеличении количества опытов значение относительной частоты все меньше и меньше отклоняется от некоторой 
константы. Скачки могут быть, но все реже и реже. Наличие этой константы называется статистической устойчивостью частот, а сама константа
вероятностью случайного события А. В случае, если все исходы опыта конечны и равновозможные, то их вероятность равна P=1/n, где n-число
возможных исходов.<br>
Пример: <br>
1.     вероятность выпадения орла при бросании монеты – 1/2.<br>
2.     вероятность вытянуть из урны красный шар (при условии, что там три шара – красный, синий, белый) – 1/3.<br>
Таким образом, когда мы имеем дело со случайными событиями, имеется некоторая неопределенность. Введем в рассмотрение численную величину,
измеряющую неопределенность опыта.<br>
<strong>Энтропия</strong> – мера неопределенности опыта, в котором проявляются случайные события. Обозначим ее H.<br>
Очевидно, что величины H  и n (число возможных исходов опыта) связаны функциональной зависимостью: H=f (n), то есть мера неопределенности
есть функция числа исходов.<br>
Какова же связь энтропии с информацией?<br>
Из определения энтропии следует, что энтропия это числовая характеристика, отражающая ту степень неопределенности, которая исчезает после
проведения опыта, то есть ПОСЛЕ ПОЛУЧЕНИЯ ИНФОРМАЦИИ. То есть, после проведения опыта получаем определенную информацию. Следовательно:
Энтропия опыта равна той информации, которую мы получаем в результате его осуществления. То есть: нформация I – это содержание сообщения,
понижающего неопределенность некоторого опыта с неоднозначным исходом; убыль связанной с ним энтропии является количественной мерой
информации.<br>
Значит,  если  H1 – начальная энтропия (до проведения опыта), H2 – энтропия после проведения опыта, то информация
I=H1-H2=log<sub>2</sub>n<sub>1</sub>-log<sub>2</sub>n<sub>2</sub>=log<sub>2</sub> (n<sub>1</sub>/n<sub>2</sub>).
Очевидно, что в случае, когда получен конкретный результат, H2=0, и, таким образом, количество полученной информации совпадает с начальной
энтропией и подсчитывается при помощи формулы Хартли. <br>
Итак, мы ввели меру неопределенности – энтропию и показали, что начальная энтропия (или убыль энтропии) равна количеству полученной в
результате опыта информации. Важным при введении какой-либо величины является вопрос о том, что принимать за единицу ее измерения. Очевидно,
значение H будет равно 1 при n=2. Иначе говоря, в качестве единицы принимается количество информации, связанное с проведением опыта,
состоящего в получении одного из двух равновероятных исходов (например, бросание монеты).  Такая единица количества информации называется
"бит".<br>
Замечание: вероятностный подход учитывает ценность информации для конкретного получателя (действительно, ведь речь идет о благоприятных
(для кого-то!!) событиях).<br>
Пример: определим количество информации, связанное с появлением каждого символа в сообщениях, записанных на русском языке. Будем считать,
что русский алфавит состоит из 33 букв и знака "пробел". По формуле Хартли: H=log<sub>2</sub>34=5 бит (здесь считаем, что появление каждой
буквы равновероятно). По формуле Шеннона (для неравновероятных исходов) это значение равно 4,72 бит. Здесь значение, полученное по формуле
Хартли – максимальное количество информации, которое моет приходиться на один знак.<br>
</p>
<p>
<strong>Меры информации</strong><br>
При реализации информационных процессов всегда происходит перенос информации в пространстве и времени от источника информации к приемнику.
При этом для передачи информации используют различные знаки или символы, например естественного или искусственного (формального) языка,
позволяющие выразить ее в форме сообщения.<br>
С точки зрения семиотики (от греч.  semeion — знак, признак) — науки, занимающейся исследованием свойств знаков и знаковых систем сообщение,
может изучаться на трех уровнях:<br>
• синтаксическом, где рассматриваются внутренние свойства сообщений, т. е. отношения между знаками, отражающие структуру данной знаковой
системы.<br>
• семантическом, где анализируются отношения между знаками и обозначаемыми ими предметами, действиями, качествами, т. е. смысловое
содержание сообщения, его отношение к источнику информации;<br>
• прагматическом, где рассматриваются отношения между сообщением и получателем, т. е. потребительское содержание сообщения, его отношение
к получателю.<br>
Таким образом, проблемы передачи информации тоже разделяют на три уровня: синтаксический, семантический и прагматический.<br>
Проблемы <strong>синтаксического уровня</strong> - это чисто технические проблемы совершенствования методов передачи сообщений и их материальных
носителей — сигналов. На этом уровне рассматривают проблемы доставки получателю сообщений как совокупности знаков, учитывая при этом тип
носителя и способ представления информации, скорость передачи и обработки, размеры кодов представления информации, надежность и точность
преобразования этих кодов и т. п. При этом полностью абстрагируются от смыслового содержания сообщений и их целевого предназначения. На
этом уровне информацию, рассматриваемую только с синтаксических позиций, обычно называют данными, так как смысловая сторона при этом не
имеет значения.<br>
Проблемы <strong>семантического уровня</strong> связаны с формализацией и учетом смысла передаваемой информации. На данном уровне анализируются те
сведения, которые отражает информация, выявляется смысл, содержание информации, осуществляется ее обобщение.
Проблемы этого уровня чрезвычайно сложны, так как смысловое содержание информации больше зависит от получателя, чем от семантики сообщения,
представленного на каком-либо языке.<br>
На прагматическом уровне интересуют последствия от получения и использования данной информации потребителем. Проблемы этого уровня связаны
с определением ценности и полезности информации для потребителя Основная сложность здесь состоит в том, что ценность, полезность информации
может быть совершенно различной для различных получателей и, кроме того, она зависит от ряда факторов, таких, например, как своевременность
ее доставки и использования. <br>
Современная теория информации исследует в основном проблемы синтаксического уровня. Она опирается на понятие «количество информации»,
которое никак не отражает ни смысла, ни важности передаваемых сообщений. В связи с этим иногда говорят, что теория информации находится
на синтаксическом уровне. <br>
Для каждого из рассмотренных выше уровней проблем передачи информации существуют свои подходы к измерению количества информации и свои
меры информации. Различают соответственно меры информации синтаксического уровня, семантического уровня и прагматического уровня. Однако
в силу вышесказанного остановимся лишь на мерах информации синтаксического уровня.<br>
Итак, количественная оценка информации этого уровня не связана с содержательной стороной информации, а оперирует с обезличенной информацией,
не выражающей смыслового отношения к объекту. В связи с этим данная мера дает возможность оценки информационных потоков в таких разных по
своей природе объектах, как системы связи, вычислительные машины, системы управления, нервная система живого организма и т. п.
Для измерения информации на синтаксическом уровне вводятся два параметра: объем информации (данных) — V (объемный подход) и количество
информации — / (вероятностный подход).<br>

<strong>Объем информации V.</strong><br>
При реализации информационных процессов информация передается в виде сообщения, представляющего собой совокупность символов какого-либо
алфавита. При этом каждый новый символ в сообщении увеличивает количество информации, представленной последовательностью символов данного
алфавита. Если теперь количество информации, содержащейся в сообщении из одного символа, принять за единицу, то объем информации (данных)
V в любом другом сообщении будет равен количеству символов (разрядов) в этом сообщении. Так как одна и та же информация может быть представлена
многими разными способами (с использованием разных алфавитов), то и единица измерения информации (данных) соответственно будет меняться.
В компьютерной технике наименьшей единицей измерения информации является 1 бит. Таким образом, объем информации, записанной двоичными знаками 
(0 и 1) в памяти компьютера или на внешнем носителе информации подсчитывается просто по количеству требуемых для такой записи двоичных символов.
Например, восьмиразрядный двоичный код 11001011 имеет объем данных V= 8 бит.<br>
В современной вычислительной технике наряду с минимальной единицей измерения данных «бит» широко используется укруп¬ненная единица измерения
«байт», равная 8 бит. При работе с большими объемами информации для подсчета ее количества применяют более крупные единицы измерения, такие как
килобайт (Кбайт), мегабайт (Мбайт), гигабайт (Гбайт), терабайт (Тбайт):<br>
1 Кбайт = 1024 байт = 2<sup>10</sup> байт;<br>
1 Мбайт = 1024 Кбайт = 2<sup>20</sup> байт = 1 048 576 байт;<br>
1 Гбайт = 1024 Мбайт = 2<sup>30</sup> байт = 1 073 741 824 байт;<br>
1 Тбайт = 1024 Гбайт = 2<sup>40</sup> байт = 1 099 511 627 776 байт.<br>
Следует обратить внимание, что в системе измерения двоичной (компьютерной) информации, в отличие от метрической системы, единицы с приставками
«кило», «мега» и т. д. получаются путем умножения основной единицы не на 103= 1000, 106= 1000 000 и т. д., а на 210 , 220 и т. д<br>
</BODY>
</HTML>